{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2-A0xZfR2RSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import random\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "6O4geIeF2Mob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74jkLETc0XcO"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = data = pd.read_csv('/content/drive/MyDrive/thesis/Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mu6srIJ0XcP"
      },
      "outputs": [],
      "source": [
        "# Separate numeric and non-numeric columns\n",
        "numeric_cols = data.select_dtypes(include=['number']).columns\n",
        "categorical_cols = data.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Impute missing values with the mean for numeric columns\n",
        "imputer_numeric = SimpleImputer(strategy='mean')\n",
        "data[numeric_cols] = imputer_numeric.fit_transform(data[numeric_cols])\n",
        "\n",
        "# Impute missing values with the most frequent value for categorical columns\n",
        "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "data[categorical_cols] = imputer_categorical.fit_transform(data[categorical_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCJVQMVm0XcP"
      },
      "outputs": [],
      "source": [
        "# Split 'charttime' into date and time components\n",
        "data[['Date', 'Time']] = data['charttime'].str.split(' ', expand=True)\n",
        "\n",
        "# Drop the 'Time' and 'charttime' columns as you only want the date component\n",
        "data.drop(['Time', 'charttime'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRz4-WpV0XcP"
      },
      "outputs": [],
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = data[numeric_cols].drop(columns=['disposition'], errors='ignore')\n",
        "y = data['disposition']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqe9MuOm0XcQ"
      },
      "outputs": [],
      "source": [
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Encode target column if necessary\n",
        "if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Convert to tensor\n",
        "X_num_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "print(X_num_tensor.shape)\n",
        "print(y_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVAh0DnE0XcQ"
      },
      "outputs": [],
      "source": [
        "# Define categorical columns\n",
        "categorical_columns = [\"gender\", \"race\", \"Date\", \"arrival_transport\", \"rhythm\", \"icd_code\", \"chiefcomplaint\", \"icd_title\", \"name\", \"etcdescription\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RhSkGff0XcQ"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for each categorical column\n",
        "word_to_index_map = {}\n",
        "encoded_data = []\n",
        "\n",
        "for column in categorical_columns:\n",
        "    # Assuming each column is a string column\n",
        "    # Extract the column values\n",
        "    column_values = data[column].tolist()\n",
        "\n",
        "    # Tokenize each value and build vocabulary\n",
        "    words = []\n",
        "    for sentence in column_values:\n",
        "        words.extend(sentence.lower().split())\n",
        "\n",
        "    word_to_index = {word: i for i, word in enumerate(set(words))}\n",
        "    word_to_index_map[column] = word_to_index\n",
        "    #print(word_to_index)\n",
        "    #print(word_to_index_map[column])\n",
        "     # Encode each value\n",
        "    encoded_column = []\n",
        "    for sentence in column_values:\n",
        "        encoded_sentence = [word_to_index[word] for word in sentence.lower().split()]\n",
        "        #print(encoded_sentence)\n",
        "        encoded_column.append(encoded_sentence)\n",
        "\n",
        "    encoded_data.append(encoded_column)\n",
        "#print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W1ubQAt0XcQ"
      },
      "outputs": [],
      "source": [
        "# Padding sequences for each column\n",
        "max_lens = [max(len(sentence) for sentence in column) for column in encoded_data]\n",
        "print(max_lens)\n",
        "padded_data = []\n",
        "\n",
        "for column, max_len in zip(encoded_data, max_lens):\n",
        "    padded_column = [sentence + [0] * (max_len - len(sentence)) for sentence in column]\n",
        "    padded_data.append(padded_column)\n",
        "    print(padded_column)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "padded_data = [torch.tensor(column) for column in padded_data]\n",
        "\n",
        "# Example of accessing word_to_index for a specific column (e.g., 'chiefcomplaint')\n",
        "print(word_to_index_map['chiefcomplaint'])\n",
        "print(word_to_index_map['icd_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPt39oBQ0XcR"
      },
      "outputs": [],
      "source": [
        "# Modify input dimension based on your data size\n",
        "input_dim = sum(len(word_to_index) for word_to_index in word_to_index_map.values())\n",
        "embedding_dim = 10  # Embedding dimension\n",
        "num_classes = 16  # Number of classes\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the target column\n",
        "encoded_targets = label_encoder.fit_transform(data[\"disposition\"])\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "targets = torch.tensor(encoded_targets, dtype=torch.long)  # Use long tensor for classification\n",
        "\n",
        "# Concatenate the padded data into a single tensor along the appropriate axis\n",
        "X_col = torch.cat(padded_data, dim=1) #data size,sum of padded length\n",
        "print(X_col[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnGt3phc0XcR"
      },
      "outputs": [],
      "source": [
        "# Convert X (categorical features) to a numpy array\n",
        "X_categorical = X_col.numpy()\n",
        "\n",
        "# If X_tensor is a tensor with more than one dimension, convert it to a numpy array\n",
        "if len(X_num_tensor.shape) > 1:\n",
        "    X_numerical = X_num_tensor.numpy()\n",
        "else:\n",
        "    # If X_tensor is a single-dimensional tensor, reshape it to a column vector\n",
        "    X_numerical = X_num_tensor.numpy().reshape(-1, 1)\n",
        "\n",
        "# Concatenate numerical and categorical features\n",
        "X_combined = np.concatenate((X_numerical, X_categorical), axis=1)\n",
        "\n",
        "# Split the data into train and test sets with stratification\n",
        "X_train_val, X_test_nn, y_train_val, y_test_nn = train_test_split(X_combined, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor)\n",
        "\n",
        "# Further split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
        "\n",
        "# Split the combined data back into numerical and categorical tensors for training\n",
        "X_train_numerical = X_train[:, :X_numerical.shape[1]]\n",
        "X_train_categorical = X_train[:, X_numerical.shape[1]:]\n",
        "\n",
        "X_val_numerical = X_val[:, :X_numerical.shape[1]]\n",
        "X_val_categorical = X_val[:, X_numerical.shape[1]:]\n",
        "\n",
        "X_test_nn_numerical = X_test_nn[:, :X_numerical.shape[1]]\n",
        "X_test_nn_categorical = X_test_nn[:, X_numerical.shape[1]:]\n",
        "\n",
        "# Convert numpy arrays back to PyTorch tensors\n",
        "X_train_numerical = torch.tensor(X_train_numerical, dtype=torch.float32)\n",
        "X_train_categorical = torch.tensor(X_train_categorical, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train.numpy(), dtype=torch.long)\n",
        "\n",
        "X_val_numerical = torch.tensor(X_val_numerical, dtype=torch.float32)\n",
        "X_val_categorical = torch.tensor(X_val_categorical, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val.numpy(), dtype=torch.long)\n",
        "\n",
        "X_test_nn_numerical = torch.tensor(X_test_nn_numerical, dtype=torch.float32)\n",
        "X_test_nn_categorical = torch.tensor(X_test_nn_categorical, dtype=torch.long)\n",
        "y_test_nn = torch.tensor(y_test_nn.numpy(), dtype=torch.long)\n",
        "\n",
        "# Check the shapes of train, validation, and test data\n",
        "print(\"Train data shapes:\", X_train_numerical.shape, X_train_categorical.shape, y_train.shape)\n",
        "print(\"Validation data shapes:\", X_val_numerical.shape, X_val_categorical.shape, y_val.shape)\n",
        "print(\"Test data shapes:\", X_test_nn_numerical.shape, X_test_nn_categorical.shape, y_test_nn.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klLAQqrf0XcR"
      },
      "outputs": [],
      "source": [
        "# Assuming X_categorical is defined\n",
        "print(\"Type of X_categorical:\", type(X_categorical))\n",
        "\n",
        "# Convert X_categorical to a numpy array if it's not already\n",
        "if not isinstance(X_categorical, np.ndarray):\n",
        "    X_categorical = np.array(X_categorical)\n",
        "\n",
        "# Now check the shape\n",
        "print(\"X_categorical shape:\", X_categorical.shape)\n",
        "\n",
        "print(\"X_numerical shape:\", X_numerical.shape)\n",
        "print(\"X_categorical shape:\", X_categorical.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TobA2Jg0XcR"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Assuming X_train_numerical, X_train_categorical, etc., are defined elsewhere\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoader objects for train, validation, and test sets\n",
        "train_dataset = TensorDataset(X_train_numerical, X_train_categorical, y_train)\n",
        "val_dataset = TensorDataset(X_val_numerical, X_val_categorical, y_val)\n",
        "test_dataset = TensorDataset(X_test_nn_numerical, X_test_nn_categorical, y_test_nn)\n",
        "\n",
        "# # Note: shuffle=True for training but keep shuffle=False for validation and test loaders for consistency\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=torch.cuda.is_available())\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=torch.cuda.is_available())\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# input_dim = sum(len(word_to_index) for word_to_index in word_to_index_map.values())\n",
        "\n",
        "# Define your model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_features=14, cat_features=77, embedding_dim=64, n_classes=8, vocab_size=input_dim):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.prelu_1 = nn.PReLU()\n",
        "        self.embedding_l1 = nn.Linear(cat_features * embedding_dim, 32)\n",
        "        self.prelu_2 = nn.PReLU()\n",
        "        self.embedding_to_add_with_n_features = nn.Linear(32, 16)\n",
        "        self.prelu_3 = nn.PReLU()\n",
        "\n",
        "        self.numerical_to_match_cat = nn.Linear(n_features, 16)\n",
        "        self.prelu_4 = nn.PReLU()\n",
        "        self.prelu_5 = nn.PReLU()\n",
        "        self.classify = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self, n=14, c=77):\n",
        "        # Process categorical features\n",
        "        c_out1 = self.embedding(c) # Shape: (batch_size, cat_features, embedding_dim)\n",
        "        c_out1 = torch.flatten(c_out1, start_dim=1)  # Flatten along all dimensions except batch, Shape: (batch_size, cat_features * embedding_dim)\n",
        "        c_out1 = self.prelu_1(c_out1)\n",
        "        c_out1 = self.embedding_l1(c_out1)  # Shape: (batch_size, 32)\n",
        "        c_out1 = self.prelu_2(c_out1)\n",
        "        c_out1 = self.embedding_to_add_with_n_features(c_out1)  # Output shape should match n_out1, Shape: (batch_size, 16)\n",
        "        c_out1 = self.prelu_3(c_out1)\n",
        "\n",
        "        # Process numerical features\n",
        "        n_out1 = self.numerical_to_match_cat(n)  # Adjust numerical feature dimensions to match, Shape: (batch_size, 16)\n",
        "        n_out1 = self.prelu_4(n_out1)  # Applying PReLU\n",
        "\n",
        "        # Combine numerical and categorical features\n",
        "        out = n_out1 + c_out1  # Ensure both have the same shape, Shape: (batch_size, 16)\n",
        "\n",
        "        out = self.prelu_5(out)  # Applying PReLU\n",
        "\n",
        "        # Final classification\n",
        "        predictions = self.classify(out)\n",
        "        return predictions\n",
        "\n",
        "# Initialize the model and move it to the GPU\n",
        "model = Model(n_features=X_train_numerical.shape[1], cat_features=X_train_categorical.shape[1], embedding_dim=64, n_classes=8).to(device)\n",
        "\n",
        "# # Define hyperparameters\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "patience = 10  # Number of epochs to wait for improvement before stopping\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize lists to store training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "early_stopping_epoch = None\n",
        "\n",
        "# Training loop with validation and early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (num_features, cat_features, labels) in enumerate(train_loader):\n",
        "        num_features, cat_features, labels = num_features.to(device), cat_features.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(num_features, cat_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for num_features, cat_features, labels in val_loader:\n",
        "            num_features, cat_features, labels = num_features.to(device), cat_features.to(device), labels.to(device)\n",
        "            outputs = model(num_features, cat_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Print average losses per epoch\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss}\")\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        early_stopping_epoch = epoch\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# # Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "pred_probs = []\n",
        "with torch.no_grad():\n",
        "    for num_features, cat_features, labels in test_loader:\n",
        "        num_features, cat_features, labels = num_features.to(device), cat_features.to(device), labels.to(device)\n",
        "        outputs = model(num_features, cat_features)\n",
        "        outputs = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        pred_probs.extend(outputs.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "pred_probs = np.array(pred_probs)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = 100 * correct / total\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "auc = roc_auc_score(y_true=all_labels, y_score=pred_probs, multi_class='ovr', average='weighted')\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m_wvP5c0XcS"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "if early_stopping_epoch is not None:\n",
        "    plt.axvline(x=early_stopping_epoch, color='red', linestyle='--', label='Early Stopping')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_plot.pdf', format='pdf', dpi=300)\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzMTzW3P0XcS"
      },
      "outputs": [],
      "source": [
        "# confusion matrix plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 14})\n",
        "plt.xlabel('Predicted labels', fontsize=16)\n",
        "plt.ylabel('True labels', fontsize=16)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.savefig('confusion_matrix_nn.pdf', format='pdf', dpi=300)\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQSl6EHu0XcS"
      },
      "outputs": [],
      "source": [
        "# Define a wrapper for the PyTorch model\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class NeuralNetClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        num_features = torch.tensor(X[:, :X_train_numerical.shape[1]], dtype=torch.float32).to(device)\n",
        "        cat_features = torch.tensor(X[:, X_train_numerical.shape[1]:], dtype=torch.long).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(num_features, cat_features)\n",
        "            outputs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "        return predicted.cpu().numpy()\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        self.model.eval()\n",
        "        num_features = torch.tensor(X[:, :X_train_numerical.shape[1]], dtype=torch.float32).to(device)\n",
        "        cat_features = torch.tensor(X[:, X_train_numerical.shape[1]:], dtype=torch.long).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(num_features, cat_features)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "        return probabilities.cpu().numpy()\n",
        "\n",
        "# Initialize the model and move it to the GPU\n",
        "# model = Model(n_features=X_train_numerical.shape[1], cat_features=X_train_categorical.shape[1], n_classes=8).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsu3PAo00XcS"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "# Define the Neural Network classifier wrapper\n",
        "nn_classifier = NeuralNetClassifier(model)\n",
        "\n",
        "\n",
        "# Define the XGBoost classifier (assuming it's already trained)\n",
        "xgb_classifier = xgb.XGBClassifier(n_estimators=100, max_depth=8, max_leaves=100, tree_method=\"hist\", enable_categorical=True, device=\"cuda\")\n",
        "print(X_train_numerical.shape)\n",
        "\n",
        "# Combine numerical and categorical features for training and testing\n",
        "X_train_combined = np.hstack((X_train_numerical, X_train_categorical))\n",
        "X_test_combined = np.hstack((X_test_nn_numerical, X_test_nn_categorical))\n",
        "print(X_train_combined.shape)\n",
        "print(y_train.shape)\n",
        "# Define the VotingClassifier with hard and soft voting\n",
        "estimators = [('xgb', xgb_classifier), ('nn', nn_classifier)]\n",
        "\n",
        "# # Hard Voting\n",
        "# hard_voting = VotingClassifier(estimators=estimators, voting='hard')\n",
        "# hard_voting.fit(X_train_combined, y_train)\n",
        "# y_pred_hard = hard_voting.predict(X_test_combined)\n",
        "# hard_voting_score = accuracy_score(y_test_nn, y_pred_hard)\n",
        "# hard_voting_precision = precision_score(y_test_nn, y_pred_hard, average='weighted')\n",
        "# hard_voting_recall = recall_score(y_test_nn, y_pred_hard, average='weighted')\n",
        "# hard_voting_f1 = f1_score(y_test_nn, y_pred_hard, average='weighted')\n",
        "# print(f\"Hard Voting Score: {hard_voting_score:.2f}\")\n",
        "\n",
        "# Soft Voting\n",
        "soft_voting = VotingClassifier(estimators=estimators, voting='soft')\n",
        "soft_voting.fit(X_train_combined, y_train)\n",
        "y_pred_soft = soft_voting.predict(X_test_combined)\n",
        "y_pred_proba = soft_voting.predict_proba(X_test_combined)\n",
        "soft_voting_score = accuracy_score(y_test_nn, y_pred_soft)\n",
        "soft_voting_precision = precision_score(y_test_nn, y_pred_soft, average='weighted')\n",
        "soft_voting_recall = recall_score(y_test_nn, y_pred_soft, average='weighted')\n",
        "soft_voting_f1 = f1_score(y_test_nn, y_pred_soft, average='weighted')\n",
        "auc = roc_auc_score(y_true=all_labels, y_score=y_pred_proba, multi_class='ovr', average='weighted')\n",
        "conf_matrix_es = confusion_matrix(y_test_nn, y_pred_soft)\n",
        "\n",
        "print(f\"Soft Voting Score: {soft_voting_score:.2f}\")\n",
        "print(f\"Precision: {soft_voting_precision:.4f}\")\n",
        "print(f\"Recall: {soft_voting_recall:.4f}\")\n",
        "print(f\"F1 Score: {soft_voting_f1:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# Print the shape of the data used to train the clf model\n",
        "# print(f\"XGBoost model trained on data with shape: {clf.get_booster().num_features()}, {len(y_train)} samples.\")\n",
        "# print(f\"Shape of training data used in clf: {X_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5JRD3_0XcS"
      },
      "outputs": [],
      "source": [
        "# confusion matrix plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_es, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('confusion_matrix_ensemble.pdf', dpi=300)\n",
        "plt.clf()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}